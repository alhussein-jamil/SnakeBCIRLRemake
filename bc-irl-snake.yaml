env_settings: {}
obs_shape:
    - 452
action_dim: 4
total_num_updates: 3906
action_is_discrete: false
num_steps: 10
num_envs: 16
device: cpu
only_eval: false
seed: 31
num_eval_episodes: 100
num_env_steps: 5000000
recurrent_hidden_state_size: 128
gamma: 0.99
log_interval: 10
eval_interval: 1000000000
save_interval: 10000000000
load_checkpoint: null
load_policy: true
resume_training: false
policy:
    _target_: imitation_learning.policy_opt.policy.Policy
    hidden_size: 128
    recurrent_hidden_size: 128
    is_recurrent: false
    obs_shape:
        - 452
    action_dim: 4
    action_is_discrete: false
    std_init: 0
policy_updater:
    _target_: imitation_learning.bc_irl.updater.BCIRL
    _recursive_: false
    use_clipped_value_loss: true
    clip_param: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.0001
    max_grad_norm: 0.5
    num_epochs: 2
    num_mini_batch: 4
    num_envs: ${num_envs}
    num_steps: ${num_steps}
    gae_lambda: 0.95
    use_gae: true
    gamma: ${gamma}
    optimizer_params:
        _target_: torch.optim.Adam
        lr: 0.0003
    batch_size: 256
    plot_interval: ${eval_interval}
    norm_expert_actions: false
    n_inner_iters: 1
    reward_update_freq: 1
    storage_cfg: ${storage}
    device: ${device}
    total_num_updates: ${total_num_updates}
    use_lr_decay: true
    get_dataset_fn:
        _target_: imitation_learning.common.utils.get_transition_dataset
        dataset_path: expert_data.pth
        env_name: ${env.env_name}
    policy_init_fn:
        _target_: imitation_learning.bc_irl.rewards.reg_init
        _recursive_: false
        policy_cfg: ${policy}
    reward:
        _target_: imitation_learning.common.net.NeuralReward
        obs_shape: ${obs_shape}
        action_dim: ${action_dim}
        reward_hidden_dim: 128
        reward_type: NEXT_STATE
        cost_take_dim: -1
        include_tanh: false
        n_hidden_layers: 2
    inner_updater:
        _target_: imitation_learning.bc_irl.differentiable_ppo.DifferentiablePPO
        _recursive_: false
        use_clipped_value_loss: true
        max_grad_norm: -1
        value_loss_coef: 0.5
        clip_param: 0.2
        entropy_coef: 0.001
        num_epochs: 2
        num_mini_batch: 4
        gae_lambda: 0.95
        use_gae: true
        gamma: 0.99
    inner_opt:
        _target_: torch.optim.Adam
        lr: 0.0001
    reward_opt:
        _target_: torch.optim.Adam
        lr: 0.0001
    irl_loss:
        _target_: torch.nn.MSELoss
        reduction: mean
storage:
    _target_: imitation_learning.policy_opt.storage.RolloutStorage
    num_steps: 10
    num_processes: 16
    recurrent_hidden_state_size: 128
    obs_shape:
        - 452
    action_dim: 4
    action_is_discrete: false
    fetch_final_obs: true
logger:
    _target_: rl_utils.logging.Logger
    _recursive_: false
    run_name: ""
    seed: ${seed}
    log_dir: ./data/vids/
    vid_dir: ./data/vids/
    save_dir: ./data/checkpoints/
    smooth_len: 10
    group_name: ""
env:
    env_name: snakie-v0
    env_settings:
        params:
            config: 
                num_exp: 20
        set_eval: false
evaluator:
    _target_: imitation_learning.common.pointmass_utils.PointMassVisualizer
    rnn_hxs_dim: ${policy.recurrent_hidden_size}
    num_render: 0
    fps: 10
    save_traj_name: null
    plt_lim: 1.5
    plt_density: 50
    agent_point_size: 60
    num_demo_plot: 10
    plot_il: true
    with_arrows: false
    plot_expert: true
    is_final_render: false
eval_args:
    policy_updater:
        reward_update_freq: -1
        n_inner_iters: 1
        use_lr_decay: true
        inner_opt:
            lr: 0.0001
    load_policy: false
    num_env_steps: 5000000
